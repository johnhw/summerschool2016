{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold Learning: Part II\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets, sklearn.linear_model, sklearn.neighbors\n",
    "import sklearn.manifold, sklearn.cluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys, os, time\n",
    "import scipy.io.wavfile, scipy.signal\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (18.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jslog import js_key_update\n",
    "# This code logs keystrokes IN THIS JUPYTER NOTEBOOK WINDOW ONLY (not any other activity)\n",
    "# Log file is ../jupyter_keylog.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "function push_key(e,t,n){var o=keys.push([e,t,n]);o>500&&(kernel.execute(\"js_key_update([\"+keys+\"])\"),keys=[])}var keys=[],tstart=window.performance.now(),last_down_t=0,key_states={},kernel=IPython.notebook.kernel;document.onkeydown=function(e){var t=window.performance.now()-tstart;key_states[e.which]=[t,last_down_t],last_down_t=t},document.onkeyup=function(e){var t=window.performance.now()-tstart,n=key_states[e.which];if(void 0!=n){var o=n[0],s=n[1];if(0!=s){var a=t-o,r=o-s;push_key(e.which,a,r),delete n[e.which]}}};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensional reduction\n",
    "A very common unsupervised learning task is *dimensional reduction*; taking a dataset with a dimension of $d_h$ and reducing to a dimension of $d_l$ which is smaller than $d_h$ but retains as much of the useful information as possible. \n",
    "\n",
    "This can be thought of as a form of lossy compression -- finding a \"simpler\" representation of the data which captures its essential properties. This of course depends upon what the \"essential properties\" that we want to keep are, but generally we want to reject *noise* and keep non-random structure. We find a **subspace** that captures the meaningful variation of a dataset.\n",
    "\n",
    "One way of viewing this process is finding *latent variables*; variables we did not directly observe, but which are simple explanations of the ones we did observe. For example, if we measure a large number of weather measurements (rainfall, pressure, humidity, windspeed), these might be a very redundant representation of a few simple variables (e.g. is there a storm?). If features correlate or cluster in the measured data we can learn this structure *even without knowing training labels*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manifold learning\n",
    "One way of looking at this problem is learning a *manifold* on which the data lies (or lies close to). A *manifold* is a geometrical structure which is locally like a low-dimensional Euclidean space. Imagine data points lying on the surface of a sheet of paper crumpled into a ball, or a 1D filament or string tangled up in a 3D space. \n",
    "\n",
    "Manifold approaches attempt to automatically find these smooth embedded structures by examining the local structure of datapoints (often by analysing the nearest neighbour graph of points). This is more flexible than linear dimensional reduction as it can in theory unravel very complex or tangled datasets. \n",
    "\n",
    "However, the algorithms are usually approximate, they do not give guarantees that they will find a given manifold, and can be computationally intensive to run. \n",
    "\n",
    "<img src=\"imgs/isomap.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self organising maps\n",
    "\n",
    "Self-organising maps are a nice half way house between clustering and manifold learning approaches. They create a dense \"net\" of clusters in the original (high-dimensional space), and force the cluster points to **also** lie in a low-dimensional space with local structure, for example, on a regular 2D grid. \n",
    "\n",
    "The algorithm causes the clusters have local smoothness in both the high and the low dimensional space; it does this by forcing cluster points on the grid to move closer (in the high-d space) to their neighbours (in the low-d grid).\n",
    "\n",
    "<img src=\"imgs/somtraining.png\"> [Image from https://en.wikipedia.org/wiki/Self-organizing_map]\n",
    "\n",
    "In other words: **clusters that are close together in the high-dimensional space should be close together in the low dimensional space**. This \"unravels\" high dimensional structure into a simple low-dimensional approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Self organising maps\n",
    "digits = sklearn.datasets.load_digits()\n",
    "digits.data -= 8.0\n",
    "print digits.data[0,:].shape, digits.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import som\n",
    "som = reload(som)\n",
    "som_map = som.SOM(48,48,64)\n",
    "som_map.learn(digits.data, epochs=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for v in [20,30,40,50]:\n",
    "    plt.figure()\n",
    "    plt.imshow(som_map.codebook[:,:,v], cmap=\"magma\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(som_map.codebook[10,10,:].reshape(8,8), cmap=\"gray\", interpolation=\"nearest\")\n",
    "plt.grid(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(32,32))\n",
    "for i in range(0,48,2):\n",
    "    for j in range(0,48,2):\n",
    "        img = som_map.codebook[i,j,:].reshape(8,8)        \n",
    "        plt.imshow(img, cmap=\"gray\", extent=[i,i+2,j,j+2])\n",
    "plt.xlim(0,48)\n",
    "plt.ylim(0,48)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The U-Matrix\n",
    "\n",
    "One very nice aspect of the self-organsing map is that we can extract the **U-matrix** which captures how close together in the **high-dimensional space** points in the low-d map are. This lets us see whether there are natural **partitions** in the layout; wrinkles in the layout that might be good clustering points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.spatial.distance\n",
    "\n",
    "def umatrix(codebook):\n",
    "    ## take the average HD distance to all neighbours within\n",
    "    ## certain radius in the 2D distance    \n",
    "    x_code, y_code = np.meshgrid(np.arange(codebook.shape[0]), np.arange(codebook.shape[1]))\n",
    "    hdmatrix = codebook.reshape(codebook.shape[0]*codebook.shape[1], codebook.shape[2])    \n",
    "    hd_distance = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(hdmatrix))**2\n",
    "    ld_distance = scipy.spatial.distance.squareform(scipy.spatial.distance.pdist(np.vstack([x_code.ravel(), y_code.ravel()]).T))\n",
    "    return np.mean(hd_distance * (np.logical_and(ld_distance>0,ld_distance<1.5)),axis=1).reshape(codebook.shape[0], codebook.shape[1])\n",
    "    \n",
    "plt.figure(figsize=(14,14))    \n",
    "um = umatrix(som_map.codebook)    \n",
    "plt.imshow(um, interpolation=\"nearest\", cmap=\"viridis\")\n",
    "plt.grid(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face-direction example\n",
    "A popular manifold learning algorithm is *ISOMAP* which uses nearest neighbour graphs to identify locally connected parts of a dataset. This examines local neighbor graphs to find an \"unraveling\" of the space to a 1D or 2D subspace, which can deal with very warped high-dimensional data, and doesn't get confused by examples like the swiss roll above (assuming parameters are set correctly!).\n",
    "\n",
    "Let's use ISOMAP (a local neighbours embedding approach) to build a real, working vision based interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a video of my head in different orientations\n",
    "face_frames = np.load(\"data/face_frames.npz\")['arr_0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show the video in opencv -- it's just a raw sequence of values\n",
    "# the video is 700 frames of 64x64 imagery\n",
    "frame_ctr = 0\n",
    "# play the video back\n",
    "while frame_ctr<face_frames.shape[1]:\n",
    "    frame = face_frames[:,frame_ctr].reshape(64,64)\n",
    "    cv2.imshow('Face video', cv2.resize(frame, (512,512), interpolation=cv2.INTER_NEAREST))\n",
    "    frame_ctr += 1\n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "    if key  == 27:\n",
    "        break\n",
    "        \n",
    "# clean up\n",
    "cv2.destroyAllWindows()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit isomap to the face data (this takes a few minutes)\n",
    "faces = face_frames.T\n",
    "isomap = sklearn.manifold.Isomap(n_neighbors=25)\n",
    "isomap.fit(faces)\n",
    "xy = isomap.transform(faces)\n",
    "orig_xy = np.array(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the following code just plots images on the plot without overlap\n",
    "overlaps = []\n",
    "\n",
    "def is_overlap(ra,rb):\n",
    "    P1X, P2X, P1Y, P2Y = ra\n",
    "    P3X, P4X, P3Y, P4Y = rb\n",
    "    \n",
    "    return not ( P2X <= P3X or P1X >= P4X or P2Y <= P3Y or P1Y >= P4Y )\n",
    "\n",
    "def overlap_test(r):\n",
    "    if any([is_overlap(r,rb) for rb in overlaps]):\n",
    "        return False\n",
    "    overlaps.append(r)\n",
    "    return True\n",
    "\n",
    "def plot_some_faces(xy, faces, thin=1.0, sz=8):\n",
    "    global overlaps\n",
    "    overlaps = []\n",
    "    q = sz/4\n",
    "    for i in range(len(xy)):\n",
    "        x, y = xy[i,0], xy[i,1]\n",
    "        image = faces[i,:].copy()\n",
    "        \n",
    "        if np.random.random()<thin:\n",
    "            for j in range(10):\n",
    "                x, y = xy[i,0], xy[i,1]\n",
    "                x += np.random.uniform(-q,q)\n",
    "                y += np.random.uniform(-q, q)\n",
    "                x *= q\n",
    "                y *= q\n",
    "                extent = [x, x+sz, y, y+sz]\n",
    "                if overlap_test(extent):                    \n",
    "                    img = image.reshape(64,64)\n",
    "                    img[:,0] = 1\n",
    "                    img[:,-1] = 1\n",
    "                    img[0,:] = 1\n",
    "                    img[-1,:] = 1                    \n",
    "                    plt.imshow(img, vmin=0, vmax=1, cmap=\"gray\",interpolation=\"lanczos\",extent=extent, zorder=100)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## make a 2D plot of the faces\n",
    "# tweak co-ordinates\n",
    "\n",
    "xy[:,0] = -orig_xy[:,0] / 2.5\n",
    "xy[:,1] = orig_xy[:,1] \n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# plot the faces\n",
    "plot_some_faces(xy, faces, sz=10)\n",
    "\n",
    "# the axes correctly\n",
    "plt.xlim(np.min(xy[:,0])-10,np.max(xy[:,0])+10)\n",
    "plt.ylim(np.min(xy[:,1])-10,np.max(xy[:,1])+10)\n",
    "plt.gca().patch.set_facecolor('gray')\n",
    "plt.xlim(-70,70)\n",
    "plt.ylim(-70,70)\n",
    "plt.grid(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_ctr = 0\n",
    "# play the video back, but show the projected dimension on the screen\n",
    "\n",
    "while frame_ctr<face_frames.shape[1]:\n",
    "    frame = face_frames[:,frame_ctr].reshape(64,64)\n",
    "    frame = (frame*256).astype(np.uint8)    \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "    xy = isomap.transform([face_frames[:,frame_ctr]])\n",
    "    cx, cy = 256, 256\n",
    "    s = 8\n",
    "    x,y = xy[0]    \n",
    "    resized_frame = cv2.resize(frame, (512,512), interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.circle(resized_frame, (int(cx-x*s), int(cy-y*s)), 10, (0,255,0), -1)\n",
    "    \n",
    "    cv2.imshow('Face video', resized_frame)\n",
    "    \n",
    "    frame_ctr += 1\n",
    "    key = cv2.waitKey(1) & 0xff\n",
    "    if key  == 27:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping UI controls to unsupervised structures\n",
    "\n",
    "The point of all of this is to find **control structures** in **sensor data**. That is, to find regularities in measured values that we could use to control a user interface.\n",
    "\n",
    "To do this, we need to map unsupervised structure onto the interface itself. We could at this point move to a supervised approach, now that we have likely candidates to target. But a simpler approach is just to hand-map unsupervised structure to controls.\n",
    "\n",
    "#### Clusters\n",
    "For example, if we have clustered a set of data (e.g. measurements of the joint angles of the hand), and extracted a set of fundamental poses, we can then create a mapping table from cluster indices to actions.\n",
    "\n",
    "|cluster | 1 | 2 | 3 | 4 |\n",
    "|-----------------------------------------------|\n",
    "|**action**  | confirm   | cancel    | increase  | decrease  |\n",
    "\n",
    "<img src=\"imgs/handposes.jpg\">\n",
    "\n",
    "\n",
    "#### Distance transform\n",
    "Sometimes it is useful to have some continuous elements in an otherwise discrete interface (e.g. to support animation on state-transitions). A useful trick is to use a **distance transform**, which takes a datapoint in the original measured space $D_H$ and returns the distances to all cluster centres. (`sklearn`'s `transform` function for certain clustering algorithms does this transformation for you)\n",
    "\n",
    "This could be used, for example, to find the top two candidates for a hand pose, and show a smooth transition between actions as the hand interpolates between them.\n",
    "\n",
    "The most obvious use of this is to **disable** any action when the distance to all clusters is too great. This implements a quiescent state and is part of solving the **Midas touch** problem; you only spend a small amount of time on a UI actively interacting and don't want to trigger actions all the time!\n",
    "\n",
    "## Manifolds\n",
    "\n",
    "In the continuous case, with a dimensional reduction approach, then the mapping can often be a simple transformation of the inferred manifold. This usually requires that the manifold be **oriented** correctly; for example, in the head pointing example, I adjusted the signs of the resulting 2D manifold to match the direction my nose points in. More generally, it might be necessary to apply a scaling or rotation of the output with a linear transform:\n",
    "\n",
    "$$ x_l = f(x_h)\\\\\n",
    "x_c = Ax_l,\n",
    "$$ where $x_l$ is the low-dimensional vector, $x_h$ is high dimensional sensor vector, $x_c$ is the vector (e.g. a cursor) we pass to the UI, and $A$ is a hand-tuned or learned transformation matrix.\n",
    "\n",
    "As an example, $A = \\begin{bmatrix}0 & 1 \\\\ -1 & 1\\end{bmatrix}$ exchanges the $x$ and $y$ co-ordinate and flips the sign of $y$.\n",
    "\n",
    "<img src=\"imgs/orienting.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Challenge\n",
    "In this practical, you will capture images from your webcam, and build a UI **control** using unlabeled data. Without providing **any** class labels or values, you have to build an interaction that can do \"something interesting\" from the image data. \n",
    "\n",
    "You have complete freedom to choose what the configuration space you want to use is; you could take images of your face or hands; take images of drawn figures; image an object rotating or moving across a surface; or anything else you want.\n",
    "\n",
    "As an illustrative example, the unsupervised approach could be used to image a soft drinks can at different rotations, and recover the rotation angle as an input (i.e. as a physical \"dial\").\n",
    "\n",
    "<img src=\"imgs/can.jpg\">\n",
    "\n",
    "The criterion is the most **interesting** but **functional** interface. The control can be discrete (using **clustering**) or continuous (using **manifold learning**). **You don't have to map the controls onto a real UI, just extract and visualise a useful signal from the image data**.\n",
    "\n",
    "The final system should be able to take a webcam image and output either a class or a (possibly $n$-dimensional) continuous value.\n",
    "\n",
    "## Tips\n",
    "\n",
    "* The webcam capture code is provided for you. `cam = Webcam()` creates a camera object and `img = cam.snap()` captures a single image from the first video device; if you have several, then you can use `cam = Webcam(1)` etc. to select the device to use. The result will be a $W\\times H\\times 3$ NumPy array, with colours **in the BGR order**.\n",
    "\n",
    "* You should resize your image (using `scipy.ndimage.zoom`) to something small (e.g. 32x48 or 64x64) so that the learning is feasible in the time available.\n",
    "\n",
    "* Your \"interface\" should probably show a 2D or 1D layout of the data in the training set, and have a mode where a new webcam image can be captured and plotted on the layout. You should consider colouring the data points by their attributes (e.g. cluster label) and/or showing some small images on the plot to get an idea of what is going on.\n",
    "\n",
    "* You can preprocess features as you like, but a good clustering/manifold learning algorithm will be able to capture much of the structure without this. **The simplicity of the processing applied will considered in judging!**; minimise the amount of hand-tweaking that you do.\n",
    "\n",
    "* Remember that some layout algorithms (e.g. t-SNE) are **unstable**. You may want to run the dimensional reduction several times and choose a good result, and use a repeatable random number seed (e.g. set it using `np.random.seed` or pass a custom `RandomState` to `sklearn`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# simple OpenCV image capture from the video device\n",
    "class Webcam(object):\n",
    "    def __init__(self, cam_id=0):\n",
    "        cap = cv2.VideoCapture(cam_id)        \n",
    "        \n",
    "    def snap(self):\n",
    "        ret, frame = cap.read()\n",
    "        return frame    \n",
    "    \n",
    "# snap(), snap(), snap()..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
